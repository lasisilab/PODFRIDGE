---
title: "Disparities in the probability of finding a relative via long-range familial search"
author: "Junhui He"
date: "`r format(Sys.time(), '%Y-%m-%d %H:%M:%S')`"
site: workflowr::wflow_site
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)

# load package
library(ggplot2)
library(ggpubr)
```

## 1 Objective

We aims to determine the difference in the probability of finding a match in a direct-to-consumer (DTC) genetic database for Black and White Americans. This analysis integrates family size distributions (from Result 2), database representation disparities (from Result 1B), and considers the proportion of DTC databases accessible to law enforcement.

We establish a theoretical binomial model $\text{Bin}(K,p)$ to calculate the probability of finding a match in a database, where $K$ is the size of the database and $p$ is the match probability between a target and another individual. The match probability $p$ is calculated based on the population size, the database size, racial representation disparities, and the family size distribution.

## 2 Model assumptions

1.  In the current generation, the population size is $N$, and the racial proportion of the population is $\alpha$.

2.  The database has $K$ individuals, and the racial proportion of the database is $\beta$. For a given race, the database is randomly sampled from the current population. Both the population proportion $\alpha$ and the database proportion $\beta$ are considered fixed constants.

3.  The family trees of different races are strictly separated. For instance, the ancestors and descendants of black Americans are also black Americans.

4.  There is no-inbreeding.

5.  The number of children per couple at the generation $g$ before the present is $r_g$. It is assumed that $r_g$ follows a multinomial distribution. In this model, the distribution of $r_1$ is estimated based on the number of children born to women aged 40-49 in 1990, the distribution of $r_2$ is estimated based on the number of children born to women aged 40-49 in 1960, and the distribution of $r_3$ is estimated based on the number of children born to women aged 70+ in 1960. For generations $g\geq 3$, $r_g$ is assumed to follow the same distribution as $r_3$. Additionally, if a specific couple is known as the ancestors of the target, it is assumed that they have at least one child.

6.  Individuals are diploid and we consider only the autosomal genome.

7.  The genome of the target individual is compared to those of all individuals in the database, and identical-by-descent (IBD) segments are identified. We assume that detectable segments must be of length $\geq$ $m$ (in Morgans). We further assume that in order to confidently detect the relationship (a "match"), we must observe at least $s$ such segments

8.  We only consider relationships for which the common ancestors have lived $g\leq g_{\max}$ generations ago. For example, $g=1$ for siblings, $g=2$ for first cousins, etc. All cousins/siblings are full.

9.  We only consider regular cousins, excluding once removed and so on.

10. The number of matches between the target and the individuals in the database is counted. If we have more than $t$ matches, we declare that there is sufficient information to trace back the target. Typically, we simply assume $t=1$.

## 3 Derivation

### 3.1 The probability of a sharing a pair of ancestor

Consider the cousins of the target. $g$ generations before the present, the target has $2^{g-1}$ ancestral couples. For example, each individual has one pair of parents ($g=1$), two pairs of grandparents ($g=2$), four pairs of great-grandparents ($g=3$) and so on. Each ancestral couple contributes to $(r_g-1) \prod_{i=1}^{g-1} r_i$ of the $(g-1)$-th cousins of the target. For example, consider a pair of grandparents ($g=2$), each individual has $(r_2-1)$ uncles/aunts, and hence $(r_2-1)r_1$ first cousins. Note that $r_g \geq 1$. Therefore, the total number of the $(g-1)$-th cousins is given by

$$\text{The number of the $(g-1)$ cousins}=2^{g-1}(r_g-1) \prod_{i=1}^{g-1} r_i.$$

Under the assumption of separated family trees and randomly sampled database, given the family size, the probability to share an ancestral couple for the first time at generation $g$ between the target and the individual in the database with the same race is approximately:

$$P(\text{first sharing a mating pair at $g$ for a certain race}|r)=\frac{2^{g-1}(r_g-1) \prod_{i=1}^{g-1} r_i}{\alpha N}.$$

### 3.2 The probability of a match given a shared mating pair

This probability is irrelevant with the race. We adopt the estimation in the paper of Erlich et al. 2018. To declare a match, we need at least $s$ segments of at least length $m$. Thus, given a first shared mating pair $g$ generations ago, the probability to observe a match is

$$P(\text{match}|g)=1-\sum_{k=0}^{s-1} \text{Bin}(k;2Lg+22,\frac{\exp(-2mg)}{2^{2g-2}}),$$ where $L$ is the total genome length, roughly $L=35$.

### 3.3 The match probability between a target and a individual from the same race

Given the family size, the probability of declaring a match between the target and a random individual in the database is simply the sum of the product over all $g$,

$$ P(\text{match}|r)=\sum_{g=1}^{g_{\max}} P(\text{match}|g) \frac{2^{g-1}(r_g-1) \prod_{i=1}^{g-1} r_i}{\alpha N}. $$

The number of matches to a database is assumed to follow a binomial distribution defined as

$$ \text{Bin}(\beta K, P(\text{match}|r)) .$$

To identify an individual, we need to find at least $t$ matches in the database. Thus, given the family size,

$$P(\text{identify}|r)=1-\sum_{k=1}^{t-1} \text{Bin}(k;\beta K, P(\text{match}|r)).$$

We utilize Monte Carlo methods to calculate the mean probability of identifying an individual over family size, i.e., $E[P(\text{identify}|r)]$. The Monte Carlo method is a popular statistical technique for numerically estimating expectations over complex distributions. Our algorithm follows these steps:

1.  Given an integer $I\in\mathbb{N}$, for each iteration $i=1,\ldots,I$ and each generation $g$, we independently sample a value of $r_g^{(i)}$ from the corresponding family size distribution (which is modeled using a zero-inflated negative binomial model). We denote $r^{(i)}$ as $\{r_g^{(i)}\}_{g=1}^{g_\max}$.

2.  For each sampled $r^{(i)}$, we compute the identifying probability $P(\text{identify}|r^{(i)})$.

3.  The expected identification probability is then estimated as: $$E[P(\text{identify}|r)]=\frac{1}{I}\sum_{i=1}^I P(\text{identify}|r^{(i)}).$$

Additionally, since $r$ is a random variable, we calculate $P(\text{identify}|\bar{r})$ to make a comparison with $E[P(\text{identify}|r)]$, where $\bar{r}$ represents the mean number of children. This probability $P(\text{identify}|\bar{r})$ is used in the paper of Erlich et al. 2018, if we substitute $\bar{r}$ by a constant $r$.

## 4 Experimental results

```{r match-prob}
# Define genome size and number of chromosomes
genome_size = 35 
num_chrs = 22

# calculate the number of the (g-1)-th cousins (view siblings as the 0-th cousins) with family size r
count_cousins <- function(g, r) {
  k_ancestor_couple = 2^(g-1)
  k_cousins = k_ancestor_couple*(r[g]-1) # remove the descendants of the p-th ancestor
  if(g>1) {
    for(i in 1:(g-1)) {
      k_cousins = k_cousins * r[i]
    }
  }
  return(k_cousins)
} 

# calculate the probability of observing a match
genetic_match <- function(g, m, min_num_seg) {
  m = m / 100 # Convert m from cM to fraction
  f = exp(-2 * g * m) / 2^(2 * g - 2) # calculate the probability of sharing a detectable IBD segment
  pr = 1 - pbinom(min_num_seg - 1, num_chrs + genome_size * 2 * g, f) # calculate probability
  return(pr)
}

# calculate the match probability of the (g-1)-th cousins
match_prob <- function(g, r, N, m, min_num_seg) {
  p_genetic = genetic_match(g, m, min_num_seg)
  p_genealogic = count_cousins(g, r) / N
  p_match = p_genetic * p_genealogic
  return(p_match)
}

# calculate the probability of find at least min_num_rel relatives up to the (gmax-1)-th cousins
coverage <- function(gmax, N, K, r, m, min_num_seg, min_num_rel) {
  if(length(r)==1) {
    r = rep(r, gmax)
  }
  tot_p_match = 0
  
  # calculate the match probability up to the (gmax-1)-th cousins
  for(g in 1:gmax) {
    tot_p_match = tot_p_match + match_prob(g, r, N, m, min_num_seg)
  }
  
  pr_success = 1 - pbinom(min_num_rel-1, K, tot_p_match) # probability of success
  return(pr_success)
}
```

```{r data}
# read racial compositions in Census and DTC database (Result 1b)
racial_composition = read.csv(file.path("output", "demographic_composition_comparison.csv"))

# population proportions
pop_props = racial_composition$Proportion[c(1,3)]
# DTC proportions
dtc_props = racial_composition$Proportion[c(10, 12)]
# population size
N = sum(racial_composition$Number_of_Customers[1:4])

# read family size multinomial distributions for black/white Americans (Result 2)
family_size = read.csv(file.path("data", "family_size.csv"))
# read estimated family distributions (zero inflated negative binomial/Poisson)
load(file.path("data", "family_distribution.Rdata"))

black_family_size = rbind(family_size$proportion[which(family_size$RACE=="Black/African American" & family_size$YEAR=="1990" & family_size$AGE_RANGE=="40-49")],
                          family_size$proportion[which(family_size$RACE=="Black/African American" & family_size$YEAR=="1960" & family_size$AGE_RANGE=="40-49")],
                          family_size$proportion[which(family_size$RACE=="Black/African American" & family_size$YEAR=="1960" & family_size$AGE_RANGE=="70+")])

white_family_size = rbind(family_size$proportion[which(family_size$RACE=="White" & family_size$YEAR=="1990" & family_size$AGE_RANGE=="40-49")],
                          family_size$proportion[which(family_size$RACE=="White" & family_size$YEAR=="1960" & family_size$AGE_RANGE=="40-49")],
                          family_size$proportion[which(family_size$RACE=="White" & family_size$YEAR=="1960" & family_size$AGE_RANGE=="70+")])
```

```{r monte-carlo}
# assuming the family size follows a multinomial distribution, calculate the distribution of match probabilities using Monte Carlo methods
coverage_samples_mul <- function(num_sample, family_size, gmax, tot_N, tot_K, pop_prop, dtc_prop, m, min_num_seg, min_num_rel) {
  N = round(tot_N * pop_prop) # population size of certain race
  K = round(tot_K * dtc_prop) # database size of certain race
  
  cov_sam = rep(NA, num_sample)
  g_family = dim(family_size)[1]
  
  for (i in 1:num_sample) {
    tot_p_match = 0
    for (g in 1:gmax) {
      # sample a family size
      r = rep(NA, g)
      if(g > 1) {
        for(k in 1:(g-1)) {
          r[k] = which.max(rmultinom(1,1,family_size[min(k,g_family),])) - 1
        }
      }
      r[g] = which.max(rmultinom(1,1,family_size[min(g,g_family),-1]))
      tot_p_match = tot_p_match + match_prob(g, r, N, m, min_num_seg)
    }
    
    # calculate coverage
    cov_sam[i] = 1 - pbinom(min_num_rel-1, K, tot_p_match)
  }
  
  return(cov_sam)
}

# assuming the family size follows a zero-inflated negative binomail/Poisson distribution, calculate the distribution of match probabilities using Monte Carlo methods
coverage_samples_zi <- function(num_sample, family_distribution, gmax, tot_N, tot_K, pop_prop, dtc_prop, m, min_num_seg, min_num_rel) {
  N = round(tot_N * pop_prop) # population size of certain race
  K = round(tot_K * dtc_prop) # database size of certain race
  
  cov_sam = rep(NA, num_sample)
  g_family = length(family_distribution)
  
  for (i in 1:num_sample) {
    tot_p_match = 0
    for (g in 1:gmax) {
      # sample a family size
      r = rep(NA, g)
      if(g > 1) {
        for(k in 1:(g-1)) {
          r_dist = family_distribution[[min(k,g_family)]]
          r_dist[1] = exp(r_dist[1]) # calculate the mean for count model
          r_dist[2] = exp(r_dist[2])/(1+exp(r_dist[2])) # calculate the probability for zero inflation model
          if(runif(1)<r_dist[2]) {
            r[k] = 0
          } else {
            if(length(r_dist)==2) {
              # zero-inflated Poisson
              r[k] = rpois(1, r_dist[1])
            } else {
              # zero-inflated negative binomial
              r[k] = rnbinom(1, size = r_dist[3], mu = r_dist[1])
            }
          }
        }
      }
      
      # the number of children should be at least one
      r_dist = family_distribution[[min(g,g_family)]]
      r_dist[1] = exp(r_dist[1])
      r_g = 0
      if(length(r_dist)==2) {
        while(r_g == 0) {
          r_g = rpois(1, r_dist[1])
        }
      } else {
        while (r_g == 0) {
          r_g = rnbinom(1, size = r_dist[3], mu = r_dist[1])
        }
      }
      r[g] = r_g
      tot_p_match = tot_p_match + match_prob(g, r, N, m, min_num_seg)
    }
    
    # calculate coverage
    cov_sam[i] = 1 - pbinom(min_num_rel-1, K, tot_p_match)
  }
  
  return(cov_sam)
}
```

We focus on long-range familial search in the United States, using the U.S. Census population as the total population size, given as $N=331,449,281$. The racial composition is $$\alpha_{black}=12.05021\%,~\alpha_{white}=57.83619\%,~\alpha_{other}=30.1136\%,$$ where $\alpha_i$ indicates the proportion of race $i$ in the population.

Our analysis examines how family size distributions and database representation influence the probability of identifying an individual in a database. We consider two scenarios for database representation:

-   Ideal case: All racial groups are proportionally represented in the database, meaning $\beta_{black}=\alpha_{black}$ and $\beta_{white}=\alpha_{white}$.

-   Real case: Black Americans are underrepresented in the database, while White Americans are overrepresented. The exact values of $\beta$ depend on the specific database available. For this analysis, we use a calibrated estimation from Stella BooydeGraaff (Result 1b), where $$\beta_{black}=3.847235\%,~\beta_{white}=80\%.$$

In our analysis, the maximum database size is set to $K=0.1*N=33,144,928$. Thus, the range of database size is $[0,0.1*N].$

### 4.1 Identifying probabilities without considering representation disparities

In this subsection, we assume that the population and database have the same representation. The database proportions $\beta$ are set equal to the population proportions $\alpha$ in the calculation of identifying probabilities. These results are utilized to explore the solely influence of disparities in the family size of different races.

```{r match-parameters-1}
N_black = round(N *  pop_props[2] / 100) #black population size 39940338
N_white = round(N * pop_props[1] / 100) #white population size 191697647
num_sample = 1e4 #number of Monte Carlo samples
K_props = seq(0,0.1,length.out=50) #database/population
Ks_black = round(N_black*K_props) #black DTC database size
Ks_white = round(N_white*K_props) #white DTC database size
m = 6 #minimal cM
min_num_seg = 2 #number of segments
min_num_rel = 1 #number of relatives
gmaxs = c(2:5) #number of generations
```

```{r identify-probability-1}
#pr_black_mul = array(dim = c(length(gmaxs), length(K_props)))
#pr_white_mul = array(dim = c(length(gmaxs), length(K_props)))
pr_black_zi = array(dim = c(length(gmaxs), length(K_props)))
pr_white_zi = array(dim = c(length(gmaxs), length(K_props)))
pr_black_pse = array(dim = c(length(gmaxs), length(K_props)))
pr_white_pse = array(dim = c(length(gmaxs), length(K_props)))
r_black = sapply(1:3, function(i) {return(sum(black_family_size[i,]*c(0:12)))})
r_white = sapply(1:3, function(i) {return(sum(white_family_size[i,]*c(0:12)))})
for(i in 1:length(K_props)) {
  for(j in 1:length(gmaxs)) {
    #cov_black_mul = coverage_samples_mul(num_sample, black_family_size, gmaxs[j], N_black, Ks_black[i], 1, 1, m, min_num_seg, min_num_rel)
    #pr_black_mul[j, i] = mean(cov_black_mul)
    #cov_white_mul = coverage_samples_mul(num_sample, white_family_size, gmaxs[j], N_white, Ks_white[i], 1, 1, m, min_num_seg, min_num_rel)
    #pr_white_mul[j, i] = mean(cov_white_mul)
    
    cov_black_zi = coverage_samples_zi(num_sample, family_distribution$black, gmaxs[j], N_black, Ks_black[i], 1, 1, m, min_num_seg, min_num_rel)
    pr_black_zi[j, i] = mean(cov_black_zi)
    cov_white_zi = coverage_samples_zi(num_sample, family_distribution$white, gmaxs[j], N_white, Ks_white[i], 1, 1, m, min_num_seg, min_num_rel)
    pr_white_zi[j, i] = mean(cov_white_zi)
    
    pr_black_pse[j, i] = coverage(gmaxs[j], N_black, round(Ks_black[i]), c(r_black, rep(r_black[3], max(0, gmaxs[j]-3))), m, min_num_seg, min_num_rel)
    pr_white_pse[j, i] = coverage(gmaxs[j], N_white, round(Ks_white[i]), c(r_white, rep(r_white[3], max(0, gmaxs[j]-3))), m, min_num_seg, min_num_rel)
  }
}
```

The probabilities of identifying an individual over the family size up to a certain type of cousinship from a database without representation disparities, where the identifying probability is $E[P(\text{identify}|r)]$ and the pseudo probability is $P(\text{identify}|\bar{r})$. The family size distributions are estimated using a zero inflation model.

From left to right, the panels in Figure 1 are:

1.  Identifying probability using zero inflation models (specifically, the better model from zero-inflated negative binomial and zero-inflated poisson)

2.  Pseudo identifying probability

```{r plot-probability-1, fig.width=7, fig.height=4}
#black_df_mul = data.frame(prob=c(pr_black_mul), prop=rep(K_props, each=length(gmaxs)), cousin=factor(rep(gmaxs-1, length(K_props))), race=factor("Black American"))
#white_df_mul = data.frame(prob=c(pr_white_mul), prop=rep(K_props, each=length(gmaxs)), cousin=factor(rep(gmaxs-1, length(K_props))), race=factor("White American"))
#prob_df_mul = rbind(black_df_mul, white_df_mul)

#p.11 = ggplot(prob_df_mul, aes(x=prop, y=prob)) + geom_line(aes(colour=cousin, linetype=race)) + xlab("Database size/population size") + ylab("Probability of identifying an individual") + ylim(0,1)

black_df_zi = data.frame(prob=c(pr_black_zi), prop=rep(K_props, each=length(gmaxs)), cousin=factor(rep(gmaxs-1, length(K_props))), race=factor("Black American"))
white_df_zi = data.frame(prob=c(pr_white_zi), prop=rep(K_props, each=length(gmaxs)), cousin=factor(rep(gmaxs-1, length(K_props))), race=factor("White American"))
prob_df_zi = rbind(black_df_zi, white_df_zi)

p.12 = ggplot(prob_df_zi, aes(x=prop*N, y=prob)) + geom_line(aes(colour=cousin, linetype=race)) + xlab("Database size") + ylab("") + ylim(0,1)

pse_black_df = data.frame(prob=c(pr_black_pse), prop=rep(K_props, each=length(gmaxs)), cousin=factor(rep(gmaxs-1, length(K_props))), race=factor("Black American"))
pse_white_df = data.frame(prob=c(pr_white_pse), prop=rep(K_props, each=length(gmaxs)), cousin=factor(rep(gmaxs-1, length(K_props))), race=factor("White American"))
pse_prob_df = rbind(pse_black_df, pse_white_df)

p.13 = ggplot(pse_prob_df, aes(x=prop*N, y=prob)) + geom_line(aes(colour=cousin, linetype=race)) + xlab("Database size") + ylab("") + ylim(0,1)

ggarrange(p.12, p.13, nrow = 1, ncol = 2, common.legend = TRUE)
```

### 4.2 Identifying probabilities considering representation disparities

In this subsection, we assume that representation disparities exist between the population and the database. Specifically, Black Americans are underrepresented, while White Americans are overrepresented in the database. We calculate the probabilities of identifying an individual by treating $\alpha$ and $\beta$ fixed as constants. When the database size is much smaller than the population size, this assumption seems to be reasonable.

```{r match-parameters-2}
N = sum(racial_composition$Number_of_Customers[1:4]) #population size
num_sample = 1e4 #number of Monte Carlo samples
K_props = seq(0,0.1,length.out=50) #database/population
Ks = round(N*K_props) #DTC database size
m = 6 #minimal cM
min_num_seg = 2 #number of segments
min_num_rel = 1 #number of relatives
gmaxs = c(2:5) #number of generations
```

```{r identify-probability-2}
#pr_black_mul = array(dim = c(length(gmaxs), length(K_props)))
#pr_white_mul = array(dim = c(length(gmaxs), length(K_props)))
pr_black_zi = array(dim = c(length(gmaxs), length(K_props)))
pr_white_zi = array(dim = c(length(gmaxs), length(K_props)))
pr_black_pse = array(dim = c(length(gmaxs), length(K_props)))
pr_white_pse = array(dim = c(length(gmaxs), length(K_props)))
r_black = sapply(1:3, function(i) {return(sum(black_family_size[i,]*c(0:12)))})
r_white = sapply(1:3, function(i) {return(sum(white_family_size[i,]*c(0:12)))})
for(i in 1:length(K_props)) {
  for(j in 1:length(gmaxs)) {
    #cov_black_mul = coverage_samples_mul(num_sample, black_family_size, gmaxs[j], N, Ks[i], pop_props[2], dtc_props[2], m, min_num_seg, min_num_rel)
    #pr_black_mul[j, i] = mean(cov_black_mul)
    #cov_white_mul = coverage_samples_mul(num_sample, white_family_size, gmaxs[j], N, Ks[i], pop_props[1], dtc_props[1], m, min_num_seg, min_num_rel)
    #pr_white_mul[j, i] = mean(cov_white_mul)
    
    cov_black_zi = coverage_samples_zi(num_sample, family_distribution$black, gmaxs[j], N, Ks[i], pop_props[2], dtc_props[2], m, min_num_seg, min_num_rel)
    pr_black_zi[j, i] = mean(cov_black_zi)
    cov_white_zi = coverage_samples_zi(num_sample, family_distribution$white, gmaxs[j], N, Ks[i], pop_props[1], dtc_props[1], m, min_num_seg, min_num_rel)
    pr_white_zi[j, i] = mean(cov_white_zi)
    
    pr_black_pse[j, i] = coverage(gmaxs[j], round(N*pop_props[2]), round(Ks[i]*dtc_props[2]), c(r_black, rep(r_black[3], max(0, gmaxs[j]-3))), m, min_num_seg, min_num_rel)
    pr_white_pse[j, i] = coverage(gmaxs[j], round(N*pop_props[1]), round(Ks[i]*dtc_props[1]), c(r_white, rep(r_white[3], max(0, gmaxs[j]-3))), m, min_num_seg, min_num_rel)
  }
}
```

The probabilities of identifying an individual over the family size up to a certain type of cousinship from a database with representation disparities, where the identifying probability is $E[P(\text{identify}|r)]$ and the pseudo probability is $P(\text{identify}|\bar{r})$. The family size distributions are estimated using a zero inflation model.

From left to right, the panels in Figure 2 are:

1.  Identifying probability using zero inflation models

2.  Pseudo identifying probability

```{r plot-probability-2, fig.width=7, fig.height=4}
#black_df_mul = data.frame(prob=c(pr_black_mul), prop=rep(K_props, each=length(gmaxs)), cousin=factor(rep(gmaxs-1, length(K_props))), race=factor("Black American"))
#white_df_mul = data.frame(prob=c(pr_white_mul), prop=rep(K_props, each=length(gmaxs)), cousin=factor(rep(gmaxs-1, length(K_props))), race=factor("White American"))
#prob_df_mul = rbind(black_df_mul, white_df_mul)

#p.21 = ggplot(prob_df_mul, aes(x=prop, y=prob)) + geom_line(aes(colour=cousin, linetype=race)) + xlab("Database size/population size") + ylab("Probability of identifying an individual") + ylim(0,1)

black_df_zi = data.frame(prob=c(pr_black_zi), prop=rep(K_props, each=length(gmaxs)), cousin=factor(rep(gmaxs-1, length(K_props))), race=factor("Black American"))
white_df_zi = data.frame(prob=c(pr_white_zi), prop=rep(K_props, each=length(gmaxs)), cousin=factor(rep(gmaxs-1, length(K_props))), race=factor("White American"))
prob_df_zi = rbind(black_df_zi, white_df_zi)

p.22 = ggplot(prob_df_zi, aes(x=prop*N, y=prob)) + geom_line(aes(colour=cousin, linetype=race)) + xlab("Database size") + ylab("") + ylim(0,1)

pse_black_df = data.frame(prob=c(pr_black_pse), prop=rep(K_props, each=length(gmaxs)), cousin=factor(rep(gmaxs-1, length(K_props))), race=factor("Black American"))
pse_white_df = data.frame(prob=c(pr_white_pse), prop=rep(K_props, each=length(gmaxs)), cousin=factor(rep(gmaxs-1, length(K_props))), race=factor("White American"))
pse_prob_df = rbind(pse_black_df, pse_white_df)

p.23 = ggplot(pse_prob_df, aes(x=prop*N, y=prob)) + geom_line(aes(colour=cousin, linetype=race)) + xlab("Database size") + ylab("") + ylim(0,1)

ggarrange(p.22, p.23, nrow = 1, ncol = 2, common.legend = TRUE)
```

### 4.3 Conclusions

1.  Without accounting for representation disparities, it is interesting to note that the expected identification probability, $E[P(\text{identify}|r)]$, is higher for Black Americans than for White Americans at lower degrees of cousinship. However, this difference diminishes as the degree of cousinship increases and eventually reverses. A potential explanation is that Black Americans, on average, have larger family sizes than White Americans, leading to higher $E[P(\text{identify}|r)]$ at lower degrees of cousinship. However, the family size distribution among Black Americans is bimodal, meaning they are significantly more likely than White Americans to have no children. As a result, at higher degrees of cousinship, $E[P(\text{identify}|r)]$ becomes lower for Black Americans compared to White Americans. A graphical explanation of this pattern is provided in Section 5.

2.  Considering representation disparities, the identification probability for White Americans is significantly higher than that for Black Americans, as black Americans are underrepresented while white Americans are overrepresented.

3.  The value of $P(\text{identify}|\bar{r})$ is notably higher than $E[P(\text{identify}|r)]$, highlighting the importance of considering family size as a complete distribution in the analysis.

## 5 A toy example for distinguishing $E[P(\text{identify}|r)]$ and $P(\text{identify}|\bar{r})$

To intuitively illustrate the impact of the full distribution of the number of children on the calculation of the identification probability, we assume that $r_g=r_0$ for all $g\geq 1$, and compute the probability $P(\text{identify}|r_0)$ for various constant values of $r_0$. Especially, we consider the scenarios where $r_0$ is set to $2$, $2.5$ and $3$.

```{r match-parameters-3}
N = sum(racial_composition$Number_of_Customers[1:4]) #population size 331449281
K_props = seq(0,0.1,length.out=50) #database/population
Ks = round(N*K_props) #DTC database size
m = 6 #minimal cM
min_num_seg = 2 #number of segments
min_num_rel = 1 #number of relatives
gmaxs = c(2:5) #number of generations
rs = c(2, 2.5, 3) #number of children
```

```{r identify-probability-3}
pr = array(dim = c(length(gmaxs), length(K_props), length(rs)))
for(i in 1:length(gmaxs)) {
  for(j in 1:length(K_props)) {
    for(k in 1:length(rs)) {
      pr[i,j,k] = coverage(gmaxs[i], N, Ks[j], rs[k], m, min_num_seg, min_num_rel)
    }
  }
}
```

```{r plot-probability-3}
indices = expand.grid(cousin=factor(gmaxs-1), prop=K_props, children_number=factor(c("r = 2", "r = 2.5", "r = 3")))
prob_df = data.frame(indices, prob=c(pr))
ggplot(prob_df, aes(x=prop, y=prob)) + geom_line(aes(colour=cousin, linetype=children_number)) + xlab("Database size/population size") + ylab("Probability of identifying an individual") + ylim(0,1)
```

Furthermore, we consider a random family size $r$ with the following distribution: $$P(r=2)=P(r=3)=50\%.$$ This implies that half of the couples have 2 children, while the other half have 3 children. The mean family size, denoted by $\bar{r}$, is given as $2.5$. Consequently, we obtain the following relationships: $$E[P(\text{identify}|r)] = 0.5*P(\text{identify}|2) + 0.5 * P(\text{identify}|3),$$ $$P(\text{identify}|\bar{r}) = P(\text{identify}|2.5).$$

**Remark:** The formula for $E[P(\text{identify}|r)]$ above implicitly assumes the number of children remains the same across generations. This assumption is made solely to simplify the expression; the correct formula would be significantly more complex. For example, assuming $g_\max=2$, the expectation can be expressed as: $$E[P(\text{identify}|r)]=0.5*0.5*P(\text{identify}|r_1=2,r_2=2)+0.5*0.5*P(\text{identify}|r_1=2,r_2=3)\\+0.5*0.5*P(\text{identify}|r_1=3,r_2=2)+0.5*0.5*P(\text{identify}|r_1=3,r_2=3).$$

```{r plot-probability-4}
pr_avg = array(dim = c(length(gmaxs), length(K_props), 2))
pr_avg[,,1] = pr[,,2]
pr_avg[,,2] = (pr[,,1] + pr[,,3])/2
indices_avg = expand.grid(cousin=factor(gmaxs-1), prop=K_props, probability=factor(c("P(Identify|mean(r))", "E[P(Identify|r)]")))
prob_df_avg = data.frame(indices_avg, prob=c(pr_avg))
ggplot(prob_df_avg, aes(x=prop, y=prob)) + geom_line(aes(colour=cousin, linetype=probability)) + xlab("Database size/population size") + ylab("Probability of identifying an individual") + ylim(0,1)
```

This figure shows that $P(\text{identify}|\bar{r})>E[P(\text{identify}|r)]$, with the difference becoming more pronounced as the degree of cousinship increases. The underlying mathematical principle responsible for this relationship is **Jensen's Inequality**.
